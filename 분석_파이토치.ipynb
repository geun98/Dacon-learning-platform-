{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop('user_id', axis = 1)\n",
    "test_data = test_data.drop('user_id', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.get_dummies(train_data)\n",
    "test_data = pd.get_dummies(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data.drop('target', axis = 1)\n",
    "y = train_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x = sc.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x,y, test_size=0.2 ,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(16,32)\n",
    "        self.fc2 = nn.Linear(32,64)\n",
    "        self.fc3 = nn.Linear(64,1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 17\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14332\\2946357090.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "y_train = y_train.to_numpy()\n",
    "y_val = y_val.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.tensor(test_data, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] - Loss: 0.6627\n",
      "Epoch [2/1000] - Loss: 0.6528\n",
      "Epoch [3/1000] - Loss: 0.6506\n",
      "Epoch [4/1000] - Loss: 0.6483\n",
      "Epoch [5/1000] - Loss: 0.6458\n",
      "Epoch [6/1000] - Loss: 0.6440\n",
      "Epoch [7/1000] - Loss: 0.6424\n",
      "Epoch [8/1000] - Loss: 0.6405\n",
      "Epoch [9/1000] - Loss: 0.6384\n",
      "Epoch [10/1000] - Loss: 0.6366\n",
      "Epoch [11/1000] - Loss: 0.6352\n",
      "Epoch [12/1000] - Loss: 0.6334\n",
      "Epoch [13/1000] - Loss: 0.6315\n",
      "Epoch [14/1000] - Loss: 0.6303\n",
      "Epoch [15/1000] - Loss: 0.6271\n",
      "Epoch [16/1000] - Loss: 0.6255\n",
      "Epoch [17/1000] - Loss: 0.6238\n",
      "Epoch [18/1000] - Loss: 0.6211\n",
      "Epoch [19/1000] - Loss: 0.6198\n",
      "Epoch [20/1000] - Loss: 0.6170\n",
      "Epoch [21/1000] - Loss: 0.6163\n",
      "Epoch [22/1000] - Loss: 0.6138\n",
      "Epoch [23/1000] - Loss: 0.6119\n",
      "Epoch [24/1000] - Loss: 0.6102\n",
      "Epoch [25/1000] - Loss: 0.6077\n",
      "Epoch [26/1000] - Loss: 0.6069\n",
      "Epoch [27/1000] - Loss: 0.6060\n",
      "Epoch [28/1000] - Loss: 0.6028\n",
      "Epoch [29/1000] - Loss: 0.6011\n",
      "Epoch [30/1000] - Loss: 0.6007\n",
      "Epoch [31/1000] - Loss: 0.5993\n",
      "Epoch [32/1000] - Loss: 0.5963\n",
      "Epoch [33/1000] - Loss: 0.5955\n",
      "Epoch [34/1000] - Loss: 0.5937\n",
      "Epoch [35/1000] - Loss: 0.5918\n",
      "Epoch [36/1000] - Loss: 0.5915\n",
      "Epoch [37/1000] - Loss: 0.5897\n",
      "Epoch [38/1000] - Loss: 0.5885\n",
      "Epoch [39/1000] - Loss: 0.5861\n",
      "Epoch [40/1000] - Loss: 0.5858\n",
      "Epoch [41/1000] - Loss: 0.5840\n",
      "Epoch [42/1000] - Loss: 0.5817\n",
      "Epoch [43/1000] - Loss: 0.5811\n",
      "Epoch [44/1000] - Loss: 0.5795\n",
      "Epoch [45/1000] - Loss: 0.5778\n",
      "Epoch [46/1000] - Loss: 0.5766\n",
      "Epoch [47/1000] - Loss: 0.5737\n",
      "Epoch [48/1000] - Loss: 0.5735\n",
      "Epoch [49/1000] - Loss: 0.5720\n",
      "Epoch [50/1000] - Loss: 0.5695\n",
      "Epoch [51/1000] - Loss: 0.5688\n",
      "Epoch [52/1000] - Loss: 0.5665\n",
      "Epoch [53/1000] - Loss: 0.5655\n",
      "Epoch [54/1000] - Loss: 0.5642\n",
      "Epoch [55/1000] - Loss: 0.5633\n",
      "Epoch [56/1000] - Loss: 0.5616\n",
      "Epoch [57/1000] - Loss: 0.5603\n",
      "Epoch [58/1000] - Loss: 0.5577\n",
      "Epoch [59/1000] - Loss: 0.5583\n",
      "Epoch [60/1000] - Loss: 0.5567\n",
      "Epoch [61/1000] - Loss: 0.5560\n",
      "Epoch [62/1000] - Loss: 0.5535\n",
      "Epoch [63/1000] - Loss: 0.5523\n",
      "Epoch [64/1000] - Loss: 0.5513\n",
      "Epoch [65/1000] - Loss: 0.5486\n",
      "Epoch [66/1000] - Loss: 0.5487\n",
      "Epoch [67/1000] - Loss: 0.5487\n",
      "Epoch [68/1000] - Loss: 0.5471\n",
      "Epoch [69/1000] - Loss: 0.5465\n",
      "Epoch [70/1000] - Loss: 0.5433\n",
      "Epoch [71/1000] - Loss: 0.5443\n",
      "Epoch [72/1000] - Loss: 0.5415\n",
      "Epoch [73/1000] - Loss: 0.5415\n",
      "Epoch [74/1000] - Loss: 0.5389\n",
      "Epoch [75/1000] - Loss: 0.5403\n",
      "Epoch [76/1000] - Loss: 0.5380\n",
      "Epoch [77/1000] - Loss: 0.5361\n",
      "Epoch [78/1000] - Loss: 0.5354\n",
      "Epoch [79/1000] - Loss: 0.5351\n",
      "Epoch [80/1000] - Loss: 0.5353\n",
      "Epoch [81/1000] - Loss: 0.5339\n",
      "Epoch [82/1000] - Loss: 0.5311\n",
      "Epoch [83/1000] - Loss: 0.5313\n",
      "Epoch [84/1000] - Loss: 0.5291\n",
      "Epoch [85/1000] - Loss: 0.5297\n",
      "Epoch [86/1000] - Loss: 0.5276\n",
      "Epoch [87/1000] - Loss: 0.5285\n",
      "Epoch [88/1000] - Loss: 0.5260\n",
      "Epoch [89/1000] - Loss: 0.5242\n",
      "Epoch [90/1000] - Loss: 0.5234\n",
      "Epoch [91/1000] - Loss: 0.5226\n",
      "Epoch [92/1000] - Loss: 0.5229\n",
      "Epoch [93/1000] - Loss: 0.5220\n",
      "Epoch [94/1000] - Loss: 0.5207\n",
      "Epoch [95/1000] - Loss: 0.5187\n",
      "Epoch [96/1000] - Loss: 0.5188\n",
      "Epoch [97/1000] - Loss: 0.5174\n",
      "Epoch [98/1000] - Loss: 0.5158\n",
      "Epoch [99/1000] - Loss: 0.5164\n",
      "Epoch [100/1000] - Loss: 0.5162\n",
      "Epoch [101/1000] - Loss: 0.5152\n",
      "Epoch [102/1000] - Loss: 0.5149\n",
      "Epoch [103/1000] - Loss: 0.5127\n",
      "Epoch [104/1000] - Loss: 0.5122\n",
      "Epoch [105/1000] - Loss: 0.5101\n",
      "Epoch [106/1000] - Loss: 0.5099\n",
      "Epoch [107/1000] - Loss: 0.5103\n",
      "Epoch [108/1000] - Loss: 0.5074\n",
      "Epoch [109/1000] - Loss: 0.5088\n",
      "Epoch [110/1000] - Loss: 0.5098\n",
      "Epoch [111/1000] - Loss: 0.5091\n",
      "Epoch [112/1000] - Loss: 0.5058\n",
      "Epoch [113/1000] - Loss: 0.5052\n",
      "Epoch [114/1000] - Loss: 0.5038\n",
      "Epoch [115/1000] - Loss: 0.5029\n",
      "Epoch [116/1000] - Loss: 0.5024\n",
      "Epoch [117/1000] - Loss: 0.5024\n",
      "Epoch [118/1000] - Loss: 0.5033\n",
      "Epoch [119/1000] - Loss: 0.5007\n",
      "Epoch [120/1000] - Loss: 0.4991\n",
      "Epoch [121/1000] - Loss: 0.4998\n",
      "Epoch [122/1000] - Loss: 0.4971\n",
      "Epoch [123/1000] - Loss: 0.4974\n",
      "Epoch [124/1000] - Loss: 0.4961\n",
      "Epoch [125/1000] - Loss: 0.4956\n",
      "Epoch [126/1000] - Loss: 0.4947\n",
      "Epoch [127/1000] - Loss: 0.4939\n",
      "Epoch [128/1000] - Loss: 0.4958\n",
      "Epoch [129/1000] - Loss: 0.4935\n",
      "Epoch [130/1000] - Loss: 0.4914\n",
      "Epoch [131/1000] - Loss: 0.4914\n",
      "Epoch [132/1000] - Loss: 0.4914\n",
      "Epoch [133/1000] - Loss: 0.4887\n",
      "Epoch [134/1000] - Loss: 0.4909\n",
      "Epoch [135/1000] - Loss: 0.4877\n",
      "Epoch [136/1000] - Loss: 0.4871\n",
      "Epoch [137/1000] - Loss: 0.4865\n",
      "Epoch [138/1000] - Loss: 0.4865\n",
      "Epoch [139/1000] - Loss: 0.4865\n",
      "Epoch [140/1000] - Loss: 0.4863\n",
      "Epoch [141/1000] - Loss: 0.4845\n",
      "Epoch [142/1000] - Loss: 0.4837\n",
      "Epoch [143/1000] - Loss: 0.4813\n",
      "Epoch [144/1000] - Loss: 0.4818\n",
      "Epoch [145/1000] - Loss: 0.4804\n",
      "Epoch [146/1000] - Loss: 0.4801\n",
      "Epoch [147/1000] - Loss: 0.4805\n",
      "Epoch [148/1000] - Loss: 0.4780\n",
      "Epoch [149/1000] - Loss: 0.4792\n",
      "Epoch [150/1000] - Loss: 0.4775\n",
      "Epoch [151/1000] - Loss: 0.4778\n",
      "Epoch [152/1000] - Loss: 0.4771\n",
      "Epoch [153/1000] - Loss: 0.4761\n",
      "Epoch [154/1000] - Loss: 0.4761\n",
      "Epoch [155/1000] - Loss: 0.4747\n",
      "Epoch [156/1000] - Loss: 0.4747\n",
      "Epoch [157/1000] - Loss: 0.4756\n",
      "Epoch [158/1000] - Loss: 0.4723\n",
      "Epoch [159/1000] - Loss: 0.4722\n",
      "Epoch [160/1000] - Loss: 0.4717\n",
      "Epoch [161/1000] - Loss: 0.4713\n",
      "Epoch [162/1000] - Loss: 0.4719\n",
      "Epoch [163/1000] - Loss: 0.4683\n",
      "Epoch [164/1000] - Loss: 0.4683\n",
      "Epoch [165/1000] - Loss: 0.4682\n",
      "Epoch [166/1000] - Loss: 0.4687\n",
      "Epoch [167/1000] - Loss: 0.4661\n",
      "Epoch [168/1000] - Loss: 0.4669\n",
      "Epoch [169/1000] - Loss: 0.4657\n",
      "Epoch [170/1000] - Loss: 0.4648\n",
      "Epoch [171/1000] - Loss: 0.4654\n",
      "Epoch [172/1000] - Loss: 0.4647\n",
      "Epoch [173/1000] - Loss: 0.4654\n",
      "Epoch [174/1000] - Loss: 0.4619\n",
      "Epoch [175/1000] - Loss: 0.4619\n",
      "Epoch [176/1000] - Loss: 0.4613\n",
      "Epoch [177/1000] - Loss: 0.4611\n",
      "Epoch [178/1000] - Loss: 0.4606\n",
      "Epoch [179/1000] - Loss: 0.4609\n",
      "Epoch [180/1000] - Loss: 0.4593\n",
      "Epoch [181/1000] - Loss: 0.4588\n",
      "Epoch [182/1000] - Loss: 0.4581\n",
      "Epoch [183/1000] - Loss: 0.4584\n",
      "Epoch [184/1000] - Loss: 0.4601\n",
      "Epoch [185/1000] - Loss: 0.4572\n",
      "Epoch [186/1000] - Loss: 0.4573\n",
      "Epoch [187/1000] - Loss: 0.4562\n",
      "Epoch [188/1000] - Loss: 0.4546\n",
      "Epoch [189/1000] - Loss: 0.4547\n",
      "Epoch [190/1000] - Loss: 0.4550\n",
      "Epoch [191/1000] - Loss: 0.4542\n",
      "Epoch [192/1000] - Loss: 0.4547\n",
      "Epoch [193/1000] - Loss: 0.4514\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14332\\1905767537.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# squeeze를 사용하여 차원을 맞춥니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ccg\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             )\n\u001b[1;32m--> 487\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\ccg\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "# GPU 사용 가능 여부를 확인하고, GPU 또는 CPU에 모델을 옮깁니다.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 지정된 에폭 수만큼 모델을 학습합니다.\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # 모델을 학습 모드로 설정합니다.\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # 미니배치 별로 데이터를 사용하여 학습합니다.\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # 그래디언트 초기화\n",
    "        outputs = model(inputs)  # 모델에 입력 데이터를 전달하여 출력을 얻습니다.\n",
    "        loss = bce(outputs.squeeze(), labels)  # 예측값과 실제값의 차이를 계산하여 손실을 구합니다.\n",
    "        loss.backward()  # 역전파 수행\n",
    "        optimizer.step()  # 옵티마이저로 모델 파라미터 업데이트\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)  # 러닝 손실 계산\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)  # 에폭별 평균 손실 계산\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}] - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# 모델 평가\n",
    "model.eval()  # 모델을 평가 모드로 설정합니다.\n",
    "with torch.no_grad():\n",
    "    x_val_tensor, y_val_tensor = x_val_tensor.to(device), y_val_tensor.to(device)\n",
    "    outputs = model(x_val_tensor)  # 검증 데이터로 모델의 예측값을 계산합니다.\n",
    "    predicted = (outputs.cpu().numpy() > 0.5).astype(int)  # 출력값을 기준으로 이진 분류를 수행합니다.\n",
    "    accuracy = (predicted == y_val).mean()  # 정확도 계산\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ccg\\AppData\\Local\\Temp\\ipykernel_14332\\3363399831.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_tensor = torch.tensor(test, dtype=torch.float32).to(device)\n"
     ]
    }
   ],
   "source": [
    "# 모델을 사용하여 예측값 계산 (테스트 데이터에 대해)\n",
    "model.eval()  # 모델을 평가 모드로 설정합니다.\n",
    "with torch.no_grad():\n",
    "    x_test_tensor = torch.tensor(test, dtype=torch.float32).to(device)\n",
    "    outputs = model(x_test_tensor)  # 테스트 데이터에 대한 모델의 예측값을 계산합니다.\n",
    "    y_pred = (outputs.cpu().numpy() > 0.5).astype(int)  # 이진 분류를 위해 임계값을 기준으로 예측값 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
